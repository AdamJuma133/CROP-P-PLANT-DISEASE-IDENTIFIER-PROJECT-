{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdamJuma133/CROP_PLANT_DISEASE_IDENTIFIER-PROJECT-/blob/main/Crop_Plant_Disease_Identifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z83tLbEFdix"
      },
      "source": [
        "# Crop Plant Disease Identifier AI Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfxyYV_VIYoR"
      },
      "source": [
        "#1. Project Structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRzXlRvHF8Pr"
      },
      "source": [
        "This project aims to develop a multiclass crop-plant-disease-identifier using a Convolutional Neural Network (CNN) model implemented in a Google Colab notebook. The dataset contains 13 different types of crops, each with healthy and unhealthy leaf classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l96ey_khFCRG",
        "outputId": "c44c8a99-9f0a-4c8a-9e36-f81db77135ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP4R2YOnIuwp"
      },
      "source": [
        "1. Exploration\n",
        "\n",
        "1.1 Design Ideation\n",
        "\n",
        "1.2 Data sourcing from Google Drive\n",
        "\n",
        "1.3 Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-vc8cgxKBkL"
      },
      "source": [
        "\n",
        "2. Model Selection\n",
        "\n",
        "2.1 Provide pre-trained models\n",
        "\n",
        "2.2 Compare performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3IBgumkKaQM"
      },
      "source": [
        "\n",
        "3. Model Training\n",
        "\n",
        "3.1 Data split\n",
        "\n",
        "3.2 Transfer learning\n",
        "\n",
        "3.3bEvaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm4NVdjxMYZh"
      },
      "source": [
        "#2. Dataset Structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEEWvYfSMjAm"
      },
      "source": [
        "\n",
        "The dataset is saved in Google Drive in a folder named PROJECT_CPDI. Within this folder, there are 13 subfolders, each containing its respective crop dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T28Nq4u1Xv_N"
      },
      "source": [
        "1. Bean\n",
        "\n",
        " 1.1 Healthy_leaf\n",
        "\n",
        " 1.2 Unhealthy_leaf\n",
        "\n",
        ". Angular_leaf_spot\n",
        "\n",
        ". Bean_blight\n",
        "\n",
        ". Bean_mosaic\n",
        "\n",
        ". Bean_rust"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xwSitG-Yt7k"
      },
      "source": [
        "2. Cabbage\n",
        "\n",
        " 2.1 Healthy_leaf\n",
        "\n",
        " 2.2 Unhealthy leaf\n",
        "\n",
        ". Cabbage_bacterial_leave\n",
        "\n",
        ". Cabbage_leaf_spot\n",
        "\n",
        ". Cabbage_pest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIzTucY0Z-GX"
      },
      "source": [
        "\n",
        "3. Cassava\n",
        "\n",
        " 3.1 Healthy_leaf\n",
        "\n",
        " 3.2 Unhealthy_leave\n",
        "\n",
        ". Cassava_mosaic\n",
        "\n",
        ". Cassava_bacterial_blight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Csx5jP5pbj68"
      },
      "source": [
        "\n",
        "4. Cucumber\n",
        "\n",
        " 4.1 Healthy_leaf\n",
        "\n",
        " 4.2 Unhealthy leaf\n",
        "\n",
        ".Cucmber_downy_mildew\n",
        "\n",
        ".Cucumber_mosaic\n",
        "\n",
        ".Cucumber_powdery_mildew\n",
        "\n",
        ".Cucumber_viral_curlbit\n",
        "\n",
        ".Cucumber_blight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBCe0KuQcVJL"
      },
      "source": [
        "\n",
        "5. Eggplant\n",
        "\n",
        " 5.1 Healthy_leaf\n",
        "\n",
        " 5.2 Unhealthy_leaf\n",
        "\n",
        ".Eggplant_flea_beetle\n",
        "\n",
        ".Eggplant_mite\n",
        "\n",
        ".Eggplant_jassid\n",
        "\n",
        ".Eggplant_nitrogen_deficiency\n",
        "\n",
        ".Eggplant_potassium_deficiency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbYua6i0c5u4"
      },
      "source": [
        "6. Groundnut Dataset\n",
        "\n",
        "   6.1 Healthy_leaf\n",
        "\n",
        "   6.2 Unhealthy_leaf\n",
        "\n",
        ". Groundnut_leaf_spot\n",
        "\n",
        ". Groundnut_rostte_virus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G334uDk8dXgK"
      },
      "source": [
        "7. Maize Dataset\n",
        "\n",
        " 7.1 Maize_healthy_leaf\n",
        "\n",
        " 7.2 Maize_unhealthy_leaf\n",
        "\n",
        ". Corn(Maize)_northern_leaf_blight\n",
        "\n",
        ". Corn(Maize)_Gray_leaf_spot\n",
        "\n",
        ". Corn(Maize)_RS_rust\n",
        "\n",
        ". Corn(Maize)_streak_virus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAmWYjbAdxeM"
      },
      "source": [
        "8. Okra Dataset\n",
        "\n",
        " 8.1 Okra_healthy_leaf\n",
        "\n",
        " 8.2 Okra_unhealthy_leaf\n",
        "\n",
        ". Okra_black_mould\n",
        "\n",
        ". Okra_powdery_mildew\n",
        "\n",
        ". Okra_early_blight\n",
        "\n",
        ". Okra_leaf_spot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-sTnD0Serkn"
      },
      "source": [
        "9. Pepper_bell Dataset\n",
        "\n",
        " 9.1 Pepper_bell_healthy_leaf\n",
        "\n",
        " 9.2 Pepper_bell_unhealthy_leaf\n",
        "\n",
        ". Pepper_bell_bacterial_spot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKfUN1UxfL20"
      },
      "source": [
        "10. Potato Dataset\n",
        "\n",
        "   10.1 Potato_healthy_leaf\n",
        "\n",
        "   10.2 Potato_unhealthy_leaf\n",
        "\n",
        ".Potato_early_blight\n",
        "\n",
        ".Potato_late_blight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NFOyhEFfv3P"
      },
      "source": [
        "11. Sorghum Dataset\n",
        "\n",
        "   11.1 Sorghum_healthy_leaf\n",
        "\n",
        "   11.1 Sorghum_unhealthy_leaf\n",
        "\n",
        ".Sorghum_leaf_spot\n",
        "\n",
        ".Sorghum_leaf_rust"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uWeLDZ-gYtn"
      },
      "source": [
        "12. Tomato Dataset\n",
        "\n",
        "   12.1 Tomato_healthy_leaf\n",
        "\n",
        "   12.2 Tomato_unhealthy_leaf\n",
        "\n",
        ". Tomato_bacterial_spot\n",
        "\n",
        ". Tomato_early_blight\n",
        "\n",
        ". Tomato_late_blight\n",
        "\n",
        ". Tomato_leaf_mould\n",
        "\n",
        ".Tomato_yellow_curlvirus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnRNFfF4hU9L"
      },
      "source": [
        "13. Watermelon Dataset\n",
        "\n",
        " 13.1 Watermelon_healthy_leaf\n",
        "\n",
        " 13.2 Watermelon_unhealthy_leaf\n",
        "\n",
        ".Watermelon_anthracnose\n",
        "\n",
        ". Watermelon_downy_mildew\n",
        "\n",
        ".Watermelon_mosaic_virus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nUtyQ9Mj2yC"
      },
      "source": [
        "# 3. Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRgTVG4gv02L"
      },
      "source": [
        "Design Ideation\n",
        "\n",
        "\n",
        "The goal of this project is to develop a multiclass crop-plant-disease identifier using a Convolutional Neural Network (CNN) model. The dataset contains 13 different types of crops, each with healthy and unhealthy leaf classes. The project will be implemented in a Google Colab notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHERpWK8wQnZ"
      },
      "source": [
        "Data Sourcing\n",
        "\n",
        "The dataset is stored in Google Drive in a folder named PROJECT_CPDI. Within this folder, there are 13 subfolders, each containing its respective crop dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d233GSFmx3xi"
      },
      "source": [
        "Data Preparation\n",
        "\n",
        "1. Connect to Google Drive:\n",
        "\n",
        "  • Mount Google Drive to access the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP9wHeuo06bf"
      },
      "source": [
        "2. Organize Dataset:\n",
        "\n",
        "  • Ensure the dataset is properly organized into subfolders for each crop type and their respective healthy and unhealthy leaf classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUK_oDZn2a1N"
      },
      "source": [
        "3. Image Data Generators:\n",
        "\n",
        " •Use ImageDataGenerator from TensorFlow/Keras to preprocess and augment the images for training and validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRuQE-vX3j04"
      },
      "source": [
        "4. Data Splitting:\n",
        "\n",
        " •Split the data into training and validation sets using an 80-20 split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F84CQmecNPrS"
      },
      "outputs": [],
      "source": [
        "# Exploration\n",
        "\n",
        "## Import Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikxiT7d0NYGG",
        "outputId": "5ead9f72-b9b9-4f13-e545-720d3eed2185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "## Connect to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Chj7MtPVNkgY"
      },
      "outputs": [],
      "source": [
        "# Set Dataset Path\n",
        "DATASET_PATH = '/ https://drive.google.com/drive/folders/1T4Ye1kEe93H9BXj-GpOfDoL1DEYXkluu '"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        },
        "id": "zKpR9FWwNy1d",
        "outputId": "ac9bd263-ded5-4997-d84c-afc1fe7981a1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/https://drive.google.com/drive/folders/1T4Ye1kEe93H9BXj-GpOfDoL1DEYXkluu '",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-0590f978a02f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Image Data Generators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_datagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m train_generator = train_datagen.flow_from_directory(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0mkeep_aspect_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     ):\n\u001b[0;32m-> 1138\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1139\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/https://drive.google.com/drive/folders/1T4Ye1kEe93H9BXj-GpOfDoL1DEYXkluu '"
          ]
        }
      ],
      "source": [
        "# Data Preparation\n",
        "\n",
        "# Image Data Generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    DATASET_PATH,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    DATASET_PATH,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oGrC3f-PFwgx"
      },
      "outputs": [],
      "source": [
        "# Set Dataset Path\n",
        "DATASET_PATH = '/https://drive.google.com/drive/folders/1T4Ye1kEe93H9BXj-GpOfDoL1DEYXkluu ' # Removed extra forward slash from path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy0qWQY--MNn"
      },
      "source": [
        "# Model Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thCRT3kU-ofD"
      },
      "source": [
        "Compare Performance\n",
        "\n",
        "\n",
        "We will compare the performance of VGG16 and ResNet50 models on the validation set. The comparison will be based on the validation accuracy and loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGnLW1Gl-Z77"
      },
      "source": [
        "Provide Pre-trained Models\n",
        "\n",
        "\n",
        "We will use pre-trained models VGG16 and ResNet50 for transfer learning. These models are widely used for image classification tasks and have shown good performance on various datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "4gjIn4n5_hTb",
        "outputId": "3a3aeaab-3390-4481-e9ef-86d96de5f0c2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'vgg16_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-024ffaf4888a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m## Compile Models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mvgg16_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mresnet50_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vgg16_model' is not defined"
          ]
        }
      ],
      "source": [
        "# Model Selection\n",
        "\n",
        "## Import Libraries\n",
        "from tensorflow.keras.applications import VGG16, ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Compile Models\n",
        "vgg16_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "resnet50_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "## Compare Performance\n",
        "history_vgg16 = vgg16_model.fit(train_generator, validation_data=validation_generator, epochs=10)\n",
        "history_resnet50 = resnet50_model.fit(train_generator, validation_data=validation_generator, epochs=10)\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "## Load Pre-trained Models\n",
        "vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "resnet50_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "## Build Models\n",
        "def build_model(base_model):\n",
        "    x = base_model.output\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    return model\n",
        "vgg16_model = build_model(vgg16_base)\n",
        "resnet50_model = build_model(resnet50_base)\n",
        "\n",
        "## Compile Models\n",
        "vgg16_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "resnet50_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "## Compare Performance\n",
        "history_vgg16 = vgg16_model.fit(train_generator, validation_data=validation_generator, epochs=10)\n",
        "history_resnet50 = resnet50_model.fit(train_generator, validation_data=validation_generator, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9XgJJXsjHaAm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5490e115-7c59-4406-c987-18535bb70434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "\n",
        "## Load Pre-trained Models\n",
        "vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "resnet50_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "id": "14wzLUaNHbYt",
        "outputId": "f23ae72c-db5f-4c87-90a2-27bc2ee7be4f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_generator' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f85acf830076>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mvgg16_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg16_base\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mresnet50_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet50_base\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-f85acf830076>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(base_model)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_generator' is not defined"
          ]
        }
      ],
      "source": [
        "## Build Models\n",
        "def build_model(base_model):\n",
        "    x = base_model.output\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    return model\n",
        "vgg16_model = build_model(vgg16_base)\n",
        "resnet50_model = build_model(resnet50_base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STFdypk8HcSD"
      },
      "outputs": [],
      "source": [
        "##Compile Models\n",
        "vgg16_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "resnet50_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIvyjSICHdGB"
      },
      "outputs": [],
      "source": [
        "# Model Selection\n",
        "\n",
        "## Import Libraries\n",
        "from tensorflow.keras.applications import VGG16, ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d7_btAFHd1w"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5QI52gFHemB"
      },
      "outputs": [],
      "source": [
        "## Load Pre-trained Models\n",
        "vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "resnet50_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cw4aTTH2Il7u"
      },
      "outputs": [],
      "source": [
        "## Build Models\n",
        "def build_model(base_model):\n",
        "    x = base_model.output\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    return model\n",
        "\n",
        "vgg16_model = build_model(vgg16_base)\n",
        "resnet50_model = build_model(resnet50_base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLVYmGLTImQ_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcGFCXpTInEk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zrfctawAIZh"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "id": "0_3cbgAOBK8e",
        "outputId": "8024ae7b-412a-4627-b527-26545e2ffd4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'train_generator' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7aa39281f72c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mvgg16_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg16_base\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mresnet50_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet50_base\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-7aa39281f72c>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(base_model)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_generator' is not defined"
          ]
        }
      ],
      "source": [
        "# Model Training\n",
        "\n",
        "## Import Libraries\n",
        "from tensorflow.keras.applications import VGG16, ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "## Build Models\n",
        "def build_model(base_model):\n",
        "    x = base_model.output\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    return model\n",
        "\n",
        "vgg16_model = build_model(vgg16_base)\n",
        "resnet50_model = build_model(resnet50_base)\n",
        "\n",
        "## Compile Models\n",
        "vgg16_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "resnet50_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "## Train Models\n",
        "history_vgg16 = vgg16_model.fit(train_generator, validation_data=validation_generator, epochs=10)\n",
        "history_resnet50 = resnet50_model.fit(train_generator, validation_data=validation_generator, epochs=10)\n",
        "\n",
        "## Evaluate the Best Model\n",
        "best_model = vgg16_model if max(history_vgg16.history['val_accuracy']) > max(history_resnet50.history['val_accuracy']) else resnet50_model\n",
        "\n",
        "# Evaluate the best model\n",
        "evaluation = best_model.evaluate(validation_generator)\n",
        "\n",
        "print(f\"Validation Loss: {evaluation[0]}\")\n",
        "print(f\"Validation Accuracy: {evaluation[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtloIAMFAOiz"
      },
      "source": [
        "Data Split\n",
        "\n",
        "The data has already been split into training and validation sets using an 80-20 split during the data preparation step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqK3Ali0Afzv"
      },
      "source": [
        "Transfer Learning\n",
        "\n",
        "We use transfer learning with pre-trained models VGG16 and ResNet50. The models are fine-tuned on our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nAzaKNMAp3G"
      },
      "source": [
        "Evaluation\n",
        "\n",
        "We evaluate the models based on their validation accuracy and loss. The best model is selected for further use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcbG29fqJpLe"
      },
      "outputs": [],
      "source": [
        "## Load Pre-trained Models\n",
        "vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "resnet50_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae38_2ByJpeR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AE8gwG3PJpoJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loevDodTJpwR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}